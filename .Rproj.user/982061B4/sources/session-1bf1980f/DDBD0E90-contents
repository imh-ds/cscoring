---
title: 'cscoring: Example'
author: "Hohjin Im"
date: "2023-11-25"
output: 
  html_document
---

```{r setup, include = FALSE}
suppressPackageStartupMessages({
  library(tidyverse)
  library(cscoring)
})
```

## Description

In the realm of psychology and other social behavioral sciences, latent constructs are often measured using scales. These scales commonly undergo psychometric validation through methods such as exploratory factor analysis (EFA), confirmatory factor analysis (CFA), and principal component analysis (PCA). It is also common to observe varying factor loadings among indicators, implying that some are *more* reflective (i.e., representative) of their latent construct than others.  

Despite this, many researchers outside of scale development or factor analysis-related studies (e.g., structural equation modeling) tend to create an unweighted composite score by simply averaging or summing the indicator items, particularly when entering just the composite models in analyses where measurement models cannot be separated from the structural models (e.g., linear regression, SPSS PROCESS model, generalized linear models, etc.). This prevalent practice in psychology overlooks the disparity in factor loadings across indicators, thereby assuming that all indicators are equally representative of the latent construct. Although this assumption may hold true for some scales, it is seldom the case in practice. In other words, for many studies, researchers unintentionally apply a factor structure on their scales that does not reflect the original development story nor have been validated.

To address this issue, this package provides a straightforward solution for researchers to generate weighted composite scores. Users can specify the indicators and choose between correlation or regression weights. This approach ensures a more accurate representation of the latent constructs, enhancing the validity of the research findings.


## Installation & Setup

This package can be installed directly from GitHub with `devtools`:

```{r, eval = FALSE}
# If `devtools` is not already installed, install from CRAN
# install.packages("devtools")
devtools::install_github("imh-ds/cscoring")

```

Then load in the necessary packages:

```{r}

# If `tidyverse` is not already installed, install from CRAN
# install.packages("tidyverse")

library(tidyverse)
library(cscoring)
```


## Usage: Composite Scoring With Correlation Weights

The composite score is a weighted average of the indicator variables. In a simple average or sum composite, each variable contributes equally to the final score. However, in a weighted average, each variable contributes proportionally to its weight. In other words, variables with higher weights have a greater influence on the composite score.  

For correlation weights, the weights are determined by the average correlation of each variable. Taking the average correlation of each indicator measures how well each variable represents the common trend among all indicators. Indicators highly correlated with others will have a higher average correlation and thus a higher weight, contributing more to the composite score calculation. This takes into account how well each variable represents the common trend among all variables. This approach provides a more nuanced representation of the latent construct, as it takes into account the varying associations between the indicators.  

By default, the `composite_score` function will use correlation weights. To use the function with correlation weights, you will only need to specify a named list of vectors. Each name in the list should represent the name of the composite score variable, and the corresponding vector should contain the names of the indicator variables.

```{r}
# Load data
data(grit)

# Specify the named list with composite names and their respective indicators
varlist <- list(extraversion = sprintf("e%01d", seq(10)),
                neuroticism = sprintf("n%01d", seq(10)),
                agreeableness = sprintf("a%01d", seq(10)),
                conscientiousness = sprintf("c%01d", seq(10)),
                openness = sprintf("o%01d", seq(10)),
                grit = sprintf("gs%01d", seq(12)))

# Calculate correlation weighted composite scores
correlation_data <- composite_score(data = grit,
                                    varlist = varlist,
                                    weight = "correlation")
```


## Usage: Composite Scoring With Regression Weights

In the context of regression weights, the weights are derived from a linear regression model. Specifically, the correlation-weighted composite score is regressed on the indicators, and the resulting standardized regression coefficients (i.e., beta coefficients) are utilized as weights. This implies that the contribution of each indicator to the composite score is commensurate with its influence on the criterion variable, which in this case is the correlation-weighted composite score. Indicators exerting a greater influence on the correlation composite score will possess larger coefficients, thereby contributing more significantly to the regression-weighted composite score.

Several key differences between the correlation-weighted and regression-weighted approaches warrant attention. Firstly, due to the potential discrepancy in rank-order between unstandardized and standardized coefficients, all indicators are standardized prior to their inclusion in the linear regression model. Secondly, the use of indicators in the linear regression model necessitates careful consideration of multicollinearity issues. High correlation among indicators can lead to multicollinearity, which may introduce bias or incorrect regression weights in the composite score computation. Lastly, as the regression weights are the beta coefficients, the proportion of variance in the composite score explained by the indicators will approach 100%.

To use the function with regression weights, you will need to specify a named list of vectors like in the case with correlation weights, and also specify `weight = "regression"`. Each name in the list should represent the name of the composite score variable, and the corresponding vector should contain the names of the indicator variables. Since the composite variables are kept the same, we'll use the previously defined `varlist` for the regression-weighted data. 

```{r}
# Calculate regression weighted composite scores
regression_data <- composite_score(data = grit,
                                   varlist = varlist,
                                   weight = "regression")
```

As an example of how different or similar the correlation and regression-weighted composites are, look at the bivariate pearson's r of the two for `grit`.

```{r, include = FALSE}
correg_r <- round(cor(correlation_data$grit,
                      regression_data$grit,
                      use = "complete.obs"), 3)
```

```{r}
round(cor(correlation_data$grit,
          regression_data$grit,
          use = "complete.obs"), 3)
```
We see that `r paste0("pearson's r = ",print(correg_r))`. By this metric, the correlation and regression-weighted composite scores seem identical. However, we'll see more differences between the two in later comparisons.

## Comparing with Unweighted Composites: Uniform Indicators

To compare with unweighted composite scores, we'll first create a general means composite where all the indicators are given equal weight.

```{r}
# Create unweighted composite scores using sum averages
sum_data <- composite_score(data = grit,
                            varlist = varlist,
                            weight = "average")
```

Now, we can again compare the correlation and regression-weighted grit composite scores with the unweighted composite score. 
```{r compare_object2, include = FALSE}
corsum_r <- round(cor(correlation_data$grit,
                      sum_data$grit,
                      use = "complete.obs"), 3)

regsum_r <- round(cor(regression_data$grit,
                      sum_data$grit,
                      use = "complete.obs"), 3)

```

```{r}
round(cor(correlation_data$grit,
          sum_data$grit,
          use = "complete.obs"), 3)

round(cor(regression_data$grit,
          sum_data$grit,
          use = "complete.obs"), 3)
```

We can see that the unweighted composite scores closely align with that of the correlation `r paste0("(r = ",print(corsum_r),")")` and regression-weighted composite scores `r paste0("(r = ",print(regsum_r),")")`. This can also be further demonstrated by examining the correlation matrix of the Big 5 personality traits and Grit for the unweighted composite scores:

```{r}
variables <- c("grit", "extraversion", "neuroticism", "openness", "conscientiousness", "agreeableness")

cor(sum_data %>% select(all_of(variables)),
    use = "complete.obs") %>% round(2)
```

We can see that `conscientiousness` is highly correlated with `grit`. This is fairly consistent with the existing literature. When we examine the correlation matrices for the weighted composite scores, the results tell a similar story:

```{r}
# Correlation matrix for correlation-weighted composites
cor(correlation_data %>% select(all_of(variables)),
    use = "complete.obs") %>% round(2)

# Correlation matrix for regression-weighted composites
cor(regression_data %>% select(all_of(variables)),
    use = "complete.obs") %>% round(2)
```

The correlations across all weighting schemas are nearly identical, indicating that the indicators contribute uniformly to the composite. This is expected for scales where indicators do not significantly vary in their reflection of the latent construct, i.e., the indicators consistently correlate on average. In such instances, researchers should anticipate minimal differences in their results, regardless of whether they employ unweighted or weighted composite scores. *(NOTE: the uniformity of indicator contributions may suggest redundancy. Researchers may consider whether their scales can be streamlined by eliminating some repetitive items. This methodological consideration, however, extends beyond the purview of this package’s discussion.)*

The differences between weighted and unweighted composite scores (i.e., the value of using weighted composites) become more pronounced when the scales have indicators that vary in their representation of the latent construct. To demonstrate this, we can artificially induce this scenario by combining indicators from other constructs.


## Comparing with Unweighted Composites: Varying Indicators

Consider a scenario where our `conscientiousness` composite includes three 'bad' items (we'll take three items from `neuroticism` as examples). `neuroticism` correlates negatively with both `conscientiousness` and `grit` and naturally, prudent researchers would reverse-code instances where individual indicators negatively correlate with the rest. However, for the purposes of this illustration, we will maintain them as they are. Let’s proceed to construct the flawed `conscientiousness` composite as follows:

```{r}
# Define a new varlist but mix-and-match openness with conscientiousness
bad_list <- list(extraversion = sprintf("e%01d", seq(10)),
                 openness = sprintf("o%01d", seq(10)),
                 agreeableness = sprintf("a%01d", seq(10)),
                 conscientiousness = c(sprintf("c%01d", seq(7)),
                                       sprintf("n%01d", c(8:10))),
                 grit = sprintf("gs%01d", seq(12)))

# Create correlation-weighted composite
correlation_data_mix <- composite_score(data = grit,
                                        varlist = bad_list,
                                        weight = "correlation")

# Create regression-weighted composite
regression_data_mix <- composite_score(data = grit,
                                       varlist = bad_list,
                                       weight = "regression")

# Create unweighted composite
sum_data_mix <- grit
for (var in names(bad_list)) {
  sum_data_mix <- sum_data_mix %>% 
    mutate(!!var := rowMeans(x = select(., bad_list[[var]]), na.rm = T))
}

# Check correlations of correlation-weighted composite with unweighted
round(cor(correlation_data_mix$conscientiousness,
          sum_data_mix$conscientiousness,
          use = "complete.obs"), 3)

# Check correlations of regression-weighted composite with unweighted
round(cor(regression_data_mix$conscientiousness,
          sum_data_mix$conscientiousness,
          use = "complete.obs"), 3)

```

Upon implementing this, we observe that the correlation between the unweighted and weighted `conscientiousness` variables decreases from approximately 0.99 to around 0.93. Although the correlations imply that these are still highly similar, to the extent that they could be considered identical variables, the difference becomes more noticeable when examining the correlation matrices.

```{r, include = FALSE}
cor_cg <- round(cor(correlation_data_mix$conscientiousness,
                    correlation_data_mix$grit,
                    use = "complete.obs"), 3)

reg_cg <- round(cor(regression_data_mix$conscientiousness,
                    regression_data_mix$grit,
                    use = "complete.obs"), 3)

sum_cg <- round(cor(sum_data_mix$conscientiousness,
                    sum_data_mix$grit,
                    use = "complete.obs"), 3)
```

```{r}
variables <- c("grit", "extraversion", "openness", "conscientiousness", "agreeableness")

cor(sum_data_mix %>% select(all_of(variables)),
    use = "complete.obs") %>% round(3)
cor(correlation_data_mix %>% select(all_of(variables)),
    use = "complete.obs") %>% round(3)
cor(regression_data_mix %>% select(all_of(variables)),
    use = "complete.obs") %>% round(3)
```

The correlation between `grit` and `conscientiousness` for the unweighted sum score is `r paste0("r = ",sum_cg)`, while it is `r paste0("r = ",cor_cg)` for correlation-weighted composites and `r paste0("r = ",reg_cg)` for regression-weighted composites. As evident, the unweighted composite's performance significantly deteriorates where the 'bad' indicators exert excessive influence on the calculation of the final composite score. Similarly, the regression-weighted composite score is more sensitive to the inclusion of 'bad' indicators because it directly biases the beta coefficients in the linear model (in this case, the 'bad' indicators have a negative impact rather than simply a muted impact. In most cases, the negative impact will be mitigated but this serves as a good example of when regression weights can be more volatile than its correlation counterpart). In contrast, the correlation-weighted composite `conscientiousness` significantly outperforms its counterparts by upweighting the 'good' indicators and downweighting the 'bad' indicators, thereby retaining a stronger correlation closer to the true association we observed earlier.


## Usage: Higher-Order Constructs

In cases where a construct has multiple dimensions (i.e., higher-order construct), the package can simply be piped in again with the dimensions as the 'indicators' for the higher-order composite.

```{r}

# Load data
data(grit)

# Specify the named list with the lower-order composite names and their respective indicators
lower_varlist <- list(consistency_interest = sprintf("gs%01d", c(2,3,5,7,8,11)),
                      perseverence_effort = sprintf("gs%01d", c(1,4,6,9,10,12)))
                      
# Specify the named list with the higher-order composite names and their respective indicators
higher_varlist <- list(grit = c("consistency_interest", "perseverence_effort"))

# Calculate weighted composite scores
higher_data <- grit %>%
  composite_score(varlist = lower_varlist,
                  weight = "correlation") %>%
  composite_score(varlist = higher_varlist,
                  weight = "correlation")
```


## Comparing with PLS-SEM Reflective and Formative Measurements

Lastly, weighted composite scoring warrants a comparison with partial least squares structural equation modeling (PLS-SEM). PLS-SEM constructs composite variables in a similar fashion, creating correlation-weighted composites when the measurement models are defined as reflective (i.e., Mode A), and regression-weighted composites when the measurement models are defined as formative (i.e., Mode B). The question arises as to the necessity of the composite scoring offered by the current package when PLS-SEM can perform the same function. The answer lies in the operation of PLS-SEM at the structural model level.

Specifically, PLS-SEM aims to maximize the variance explained between composite variables at the structural model level and adjusts its indicator weights accordingly. As a result, PLS-SEM has a higher tendency to encounter overfitting issues compared to other analyses. In most academic research scenarios, overfitting is not a concern as the associations between pathways are typically not substantial enough to warrant this issue. However, in certain areas of attitudinal-behavioral research (e.g., intent -> behavior) and industry research (e.g., consumer experience surveys) where predictor and outcome variables are expected to correlate highly, overfitting can become a significant concern.

This concern is exemplified in the current example using data on Big 5 personality traits and Grit. Recent research in this field suggests that grit and the conscientiousness personality trait are, at best, overlapping constructs, and at worst, identical constructs. The correlation between the unweighted conscientiousness and grit composite scores exceeded 0.6, indicating empirical overlap, especially considering that these associations are at the respondent level, not the aggregate level. If grit and conscientiousness are indeed overlapping constructs, we would expect PLS-SEM to display even stronger associations as the analysis aims to maximize this association. When associations in PLS-SEM become excessively strong, it can be inferred that the analysis is overfitting the model. This overfitting, akin to the ‘overfitting’ concern in machine learning where noise is mistakenly identified as real patterns, can result in an overemphasis on the association between one predictor and the outcome, thereby diverting the effects from all other viable predictors.

```{r}

# If not in library, install `seminr` from CRAN
# install.packages("seminr")

# Load in the package `seminr`
library(seminr)

# Set measurements for PLS-SEM correlation weights
measurements_cor <- constructs(
  composite("extraversion", multi_items("e", seq(10)), mode_A),
  composite("openness", multi_items("o", seq(10)), mode_A),
  composite("neuroticism", multi_items("n", seq(10)), mode_A),
  composite("conscientiousness", multi_items("c", seq(10)), mode_A),
  composite("agreeableness", multi_items("a", seq(10)), mode_A),
  composite('grit', multi_items("gs", seq(12)), mode_A)
)

# Set structure path
structure <- relationships(
  paths(from = c("extraversion", "openness", "neuroticism", "conscientiousness", "agreeableness"),
        to = "grit")
)

# Run PLS-SEM for reflective correlation weights
plssem_cor <- seminr::estimate_pls(data = grit,
                                   measurement_model = measurements_cor,
                                   structural_model = structure)

# Save PLS-SEM correlation weighted composite scores
plssem_cordata <- as.data.frame(plssem_cor$construct_scores)

# Examine correlation matrix for composite scores
cor(plssem_cordata) %>% round(3)

```

As observed, the correlation matrix for the PLS-SEM correlation-weighted reflective composites reveals that the association between `conscientiousness` and `grit` is significantly higher compared to when we utilized the unweighted composite scores. This is a direct consequence of how PLS-SEM operates--it maximizes the variance explained in the outcome at the structural model by adjusting the weighting schema of the indicators of the predictor composites.

In industry settings, this is particularly beneficial as it helps to identify of areas businesses can invest development in to improve a key performance indicator (KPI). Likewise, in academic research, this is especially useful when the research objective is to build a prediction model to aid in the development and refinement of scales and theories by pinpointing which dimensions of a construct are most likely to be conceptual antecedents of the outcome of interest.

However, issues arise in scenarios such as the one demonstrated, where the associations between predictor and outcome variables become overly optimized, leading to overfitting. This problem also surfaces for regression-weighted formative composites in PLS-SEM:

```{r}

# Set measurements for PLS-SEM regression weights
measurements_reg <- constructs(
  composite("extraversion", multi_items("e", seq(10)), mode_B),
  composite("openness", multi_items("o", seq(10)), mode_B),
  composite("neuroticism", multi_items("n", seq(10)), mode_B),
  composite("conscientiousness", multi_items("c", seq(10)), mode_B),
  composite("agreeableness", multi_items("a", seq(10)), mode_B),
  composite('grit', multi_items("gs", seq(12)), mode_B)
)

# Run PLS-SEM for formative regression weights
plssem_reg <- seminr::estimate_pls(data = grit,
                                   measurement_model = measurements_reg,
                                   structural_model = structure)

# Save PLS-SEM regression weighted composite scores
plssem_regdata <- as.data.frame(plssem_reg$construct_scores)

# Examine correlation matrix for composite scores
cor(plssem_regdata) %>% round(3)

```

As evident, the overfitting issue becomes significantly more pronounced when employing regression-weighted formative composite variables. In instances where PLS-SEM may not be a suitable option due to overfitting concerns, one might consider covariance-based SEM (CB-SEM) as an alternative. This can be implemented through R packages such as `lavaan` or specialized software like STATA or SPSS AMOS. However, despite its widespread use in social and behavioral sciences, CB-SEM has several limitations not well understood by its everyday users.

For example, the emphasis on goodness of fit in CB-SEM often leads researchers to make ill-guided decisions. Conceptually sound models may be prematurely dismissed based on insufficient empirical fit. Often, this is not a reflection of the model’s theoretical foundation, but rather the result of measurement decisions, such as having an excessive number of indicators, an overly complex measurement model, or the use of poorly developed scales. Models that represent partial snapshots of a much larger, but unmeasured, theoretical model may also yield inadequate empirical fits due to the omission of certain covariates that could otherwise enhance the fit and compensate other latent constructs. Consequently, conceptually valid models are frequently misinterpreted and dismissed by researchers and reviewers, despite their potential contributions to the scientific literature.

On the other hand, a poorly conceptualized model may misleadingly meet empirical field standards for 'good fit' by manipulating the model at the measurement level. This could involve using a minimal number of indicators per latent construct, thereby limiting the degrees of freedom to artificially inflate the goodness of fit. In such cases, a conceptually nonsensical model can misleadingly appear as an empirically valid model. Although the discussion of how the social and behavioral sciences misinterpret and incorrectly use powerful tools like CB-SEM extends beyond the scope of the current package, these are just a few examples of situations where one might prefer not to use PLS-SEM or CB-SEM, but still require a more rigorous construction of composite variables than simply creating unweighted means.
